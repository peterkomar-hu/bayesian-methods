\newpage
\section{Graphical models}


\subsection{Elements}
\begin{itemize}
	\item $P(x, y) = P(y\;|\;x) P(x)$ is represented by 
	\begin{figure}[h]
	\centering
		\includegraphics[height=4mm]{./figs/04-xy.pdf} 
	\end{figure}

	\item $P(x, y, z) = P(y\;|\;z,x) P(z\;|\;x) P(x)$. \\ 
	 If $P(y\;|\;z,x) = P(y\;|\;z)$, then it is represented by a {\bf chain}

	\begin{figure}[h]
	\centering
		\includegraphics[height=4mm]{./figs/04-xzy.pdf} 
	\end{figure}

	Note: $y \independent x \;|\;z$, but $y \not\independent x\; | \; \emptyset$.

	\item $P(x, y_1, y_2) = P(y_1, y_2\;|\;x) P(x)$. \\
	If $P(y_1, y_2 \;|\; x) = P(y_1\;|\;x) P(y_2\;|\;x)$, then it is represented by a {\bf fork}

	\begin{figure}[h]
	\centering
		\includegraphics[height=15mm]{./figs/04-xy1y2.pdf} 
	\end{figure}

	Note: $y_1 \independent y_2 \;|\;x$, but $y_1 \not\independent y_2\; | \; \emptyset$.

	\item $P(x_1, x_2, y) = P(y\;|\;x_1, x_2) P(x_1, x_2)$. \\
	If $P(x_1, x_2) = P(x_1) P(x_2)$, then it is represented by a {\bf collider}

	\begin{figure}[h!]
	\centering
		\includegraphics[height=15mm]{./figs/04-x1x2y.pdf} 
	\end{figure}

	Note: $x_1 \independent x_2\;|\;\emptyset$, but $x_1 \not\independent x_2\;|\;y$ (!).
	
\end{itemize}

\no Examples
\begin{itemize}
	\item $P(a, b, c, d, e) = P(a)P(d) P(b\;|\;a,d) P(c\;|\;b) P(e\;|\;c) P(f\;|\;c)$ is represented by 

	\begin{figure}[h!]
	\centering
		\includegraphics[height=15mm]{./figs/04-abcdef.pdf} 
	\end{figure}

	\item $P(a, b, c, d, e, f) = P(a) P(c\;|\;a) P(b\;|\;a) P(f\;|\;c) P(d\;|\;b, c) P(e\;|\;d)$ is represented by

	\begin{figure}[h!]
	\centering
		\includegraphics[height=20mm]{./figs/04-abcdef2.pdf} 
	\end{figure}
\end{itemize}

\newpage
\subsection{Real-life examples}
\begin{itemize}
	\item Fire causes Smoke, Smoke causes Alarm to set off, but given Smoke, there's no correlation between Fire and Alarm, i.e. $\text{Fire}\;\independent\;\text{Alarm}\;|\;\text{Smoke}$. This is represented by a chain
	\begin{figure}[h!]
	\centering
		\includegraphics[height=2.8mm]{./figs/04-fire-smoke-alarm.pdf} 
	\end{figure}

	\item Both rain and the Sprinkler can cause the formation of a Puddle, they are however independent (until we observe the Puddle), i.e. $\text{Rain}\;\independent\;\text{Sprinkler}\;|\;\emptyset$. This is represented by a collider
	\begin{figure}[h!]
	\centering
		\includegraphics[height=14mm]{./figs/04-rain-puddle-sprinkler.pdf} 
	\end{figure}

	\item Heat causes both Ice Cream sales and Crime to increase, but once we know there was a heatwave, they become independent, i.e. $\text{Crime}\;\independent\;\text{Ice Cream}\;|\;\text{Heat}$. This is represented by a fork
	\begin{figure}[h!]
	\centering
		\includegraphics[height=12mm]{./figs/04-heat-icecream-crime.pdf} 
	\end{figure}

	\item Education affects Political View, which affects both Party membership and Voting behavior. This can be represented as
	\begin{figure}[h!]
	\centering
		\includegraphics[height=16mm]{./figs/04-education-vote.pdf} 
	\end{figure}

	\item Education and Experience both affect Salary, but Education also affects Experience. This can be represented as
	\begin{figure}[h!]
	\centering
		\includegraphics[height=20mm]{./figs/04-education-salary.pdf} 
	\end{figure}
\end{itemize}

\subsection{Plate notation}

\begin{figure}[h!]
\centering
	\includegraphics[height=30mm]{./figs/04-plate-notation.pdf}
\end{figure}

\newpage
\subsection{Hierarchical models}

\no {\bf Beta-Binomial model}
\begin{itemize}
	\item Data: $D = \{(k_i, n_i)\}_{i=1}^N$, where $k_i$(successes) $\in\{0, 1, \ldots n_i\}$, and $n_i$(attempts) $\in \mathds{N}$

	\item Model: 
		\begin{itemize}
			\item level 1: the parameters, $p = \{p_i\}_{i=1}^N$, where $p_i \in [0,1]$, define $P(k_i\;|\;n_i, p_i) = \text{Binomial}(k_i\;|\;n_i, p_i)$
			\item level 2: the parameters, $\alpha, \beta$ (both $>0$), define $P(p_i\;|\;\alpha, \beta) = \text{Beta}(p_i\;|\;\alpha, \beta)$.
		\end{itemize}
		This hierarchy can be represented as
		\begin{figure}[h!]
		\centering
			\includegraphics[height=26mm]{./figs/04-BetaBinomial.pdf}
		\end{figure}

	\item Joint likelihood:
	\be
		P(D, p\;|\;\alpha,\beta) = \prod_{i=1}^N\Big[\text{Binom}(k_i\;|\;n_i, p_i) \;\text{Beta}(p_i\;|\;\alpha, \beta)\Big]
	\ee

	\item Marginal likelihood:
	\ba
		P(D\;|\;\alpha, \beta) &=& \prod_{i=1}^N\left[\int\!dp_i\,\text{Binom}(k_i\;|\;n_i, p_i) \;\text{Beta}(p_i\;|\;\alpha, \beta)\right] = \prod_{i=1}^N\Big[\text{Beta-Binom}(k_i\;|\;n_i, \alpha, \beta)\Big]
		\\
		\text{where}&&
		\text{Beta-Binom}(k\;|\;n, \alpha, \beta) = \frac{\Gamma(n+1)\Gamma(\alpha + \beta)}{\Gamma(n + \alpha + \beta)} \frac{\Gamma(k+\alpha)}{\Gamma(k+1) \Gamma(\alpha)} \frac{\Gamma(n - k + \beta)}{\Gamma(n-k + 1) \Gamma(\beta)}
	\ea
\end{itemize}

\no {\bf Gamma-Poisson model} (aka. Negative Binomial model)
\begin{itemize}
	\item Data: $D = \{k_i\}_{i=1}^N$, where $k_i$(events) $\in \mathds{N}$.
	\item Model: 
	\begin{itemize}
		\item level 1: the parameters $\lambda = \{\lambda_i\}_{i=1}^N$, where $\lambda_i$ > 0, define $P(k_i\;|\;\lambda) = \text{Poisson}(k_i\;|\;\lambda_i)$
		\item level 2: the parameters $\alpha, \beta$ (both $>0$), define $P(\lambda_i\;|\; \alpha, \beta) = \text{Gamma}(\lambda_i\;|\;\alpha, \beta)$.
	\end{itemize}
	This hierarchy can be represented as 
	\begin{figure}[h!]
		\centering
			\includegraphics[height=26mm]{./figs/04-GammaPoisson.pdf}
		\end{figure}
	\item Joint likelihood:
	\be
		P(D, \lambda\;|\;\alpha, \beta) = \prod_{i=1}^N\Big[\text{Poisson}(k_i\;|\;\lambda_i) \;\text{Gamma}(\lambda_i\;|\;\alpha, \beta)\Big]
	\ee

	\item Marginal likelihood:
	\ba
		P(D\;|\;\alpha, \beta) &=& \prod_{i=1}^N\left[\int\!d\lambda_i\,\text{Poisson}(k_i\;|\;\lambda_i) \;\text{Gamma}(\lambda_i\;|\;\alpha, \beta)\right] = \prod_{i=1}^N\Big[\text{Gamma-Poisson}(k_i\;|\;\alpha, \beta)\Big]
		\\
		\text{where} && \text{Gamma-Poisson}(k\;|\;\alpha, \beta) = \frac{\Gamma(k + \alpha)}{\Gamma(k+1)\Gamma(\alpha)} \left(\frac{1}{\beta + 1}\right)^k \left(\frac{\beta}{\beta + 1}\right)^\alpha = 
		\\
		&=&
		\text{NegativeBinom}(k\;|\;r, p) = {k + r -1 \choose k} p^k (1-p)^r,\quad \text{with }r = \alpha, \; p = \frac{1}{\beta + 1}
	\ea
\end{itemize}

\no {\bf Dirichlet-Multinomial model}
\begin{itemize}
	\item Data: $D = \{k_i\in\mathds{N}^M\}_{i=1}^N = \{(k_{i,1}, k_{i,2}, \ldots k_{i,M})\}_{i=1}^N$, where $k_{i,j}$(number of outcome $j$) $\in \mathds{N}$
	\item Model:
	\begin{itemize}
		\item level 1: the parameters $p = \{p_i \in \mathds{R}^{M}\}_{i=1}^N = \{(p_{i,1}, p_{i,2},\ldots p_{i,M})\}_{i=1}^N$, where $p_{i,j}$(probability of outcome $j$ in sample $i$) $>0$, and $\sum_{j=1}^M p_{i,j} = 1,\;\forall i$, define $P(k_i\;|\;p_i) = \text{Multinomial}(k_i\;|\;k_{i,\text{tot}},p_i)$, where $k_{i,\text{tot}} = \sum_{j=1}^M k_{i,j}$
		\item level 2: the parameters $\alpha = (\alpha_1, \alpha_2, \ldots \alpha_M)$, where each $\alpha_j > 0$, define $P(p_i\;|\; \alpha)= \text{Dirichlet}(p_i\;|\;\alpha)$
	\end{itemize}
	This hierarchy can be represented as 
	\begin{figure}[h!]
		\centering
			\includegraphics[height=26mm]{./figs/04-DirichletMultinomial.pdf}
		\end{figure}
	\item Joint likelihood:
	\be
		P(D\;p|\;\alpha) = \prod_{i=1}^N\Big[ \text{Multinomial}(k_i\;|\;k_{i,\text{tot}}, p_i)\;\text{Dirichlet}(p_i\;|\alpha) \Big]
	\ee
	\item Marginal likelihood:
	\ba
		P(D\;|\;\alpha) &=& \prod_{i=1}^N\left[\int\!dp_i\, \text{Multinomial}(k_i\;|\;k_{i,\text{tot}}, p_i)\;\text{Dirichlet}(p_i\;|\alpha) \right] = \prod_{i=1}^N\Big[\text{Dirichlet-Multinomial}(k_i\;|\;k_{i,\text{tot}}, \alpha)\Big]
		\\
		\text{where} && \text{Dirichlet-Multinomial}(k\;|\;k_\text{tot}, \{\alpha_j\}_{j=1}^M) = \frac{\Gamma(k_\text{tot}+1)\Gamma(\alpha_\text{tot})}{\Gamma(k_\text{tot} + \alpha_\text{tot})} \prod_{j=1}^M \frac{\Gamma(k_j + \alpha_j)}{\Gamma(k_j + 1)\Gamma(\alpha_j)},
		\\
		&&\text{where }\quad \alpha_\text{tot} = \sum_{j=1}^M \alpha_j
	\ea
\end{itemize}

\newpage
\no {\bf Random Effect Model}
\begin{itemize}
	\item Data: $D = \{x_g\}_{g=1}^G = \{(x_{g,1}, x_{g,2},\ldots x_{g, N_g})\}_{g=1}^G$, where $x_{g,i} \in \mathds{R}$ is measurement $i$ in group $g$, and groups can be of different sizes $N_g$.
	\item Model:
	\begin{itemize}
		\item level 1: The parameters $\mu = \{\mu_g\}_{g=1}^G$ and $\sigma^2$ define $P(x_{g,i}\;|\;\mu_g, \sigma) = \text{Normal}(x_{g,i}\;|\;\mu_g, \sigma^2)$
		\item level 2: The parameter $\sigma_0^2$ define $P(\mu_g\;|\;\mu_0,\sigma_0) = \text{Normal}(\mu_g\;|\;\mu_0, \sigma_0^2)$
	\end{itemize}
	This hierarchy can be represented as  
	\begin{figure}[h]
		\centering
			\includegraphics[height=26mm]{./figs/04-anova.pdf}
	\end{figure}
	\item Joint likelihood:
	\be
		P(D, \mu\;|\; \mu_0, \sigma_0, \sigma) = \prod_{g=1}^G\left[\text{Normal}(\mu_g\;|\;\mu_0, \sigma_0^2) \times \prod_{i=1}^{N_g}\text{Normal}(x_{g,i}\;|\;\mu_g, \sigma^2)\right]
	\ee
	\item Marginal likelihood:
	\ba
		P(D\;|\;\mu_0, \sigma_0, \sigma) 
		&=& 
		\prod_{g=1}^G\intop_{-\infty}^{+\infty}\!d\mu_g\,\left[\text{Normal}(\mu_g\;|\;\mu_0, \sigma_0^2) \times \prod_{i=1}^{N_g}\text{Normal}(x_{g,i}\;|\;\mu_g, \sigma^2)\right] 
		\\
		&=&
		(2\pi \sigma_0^2)^{-\frac{G}{2}} \prod_{g=1}^G \left[\frac{1}{\sqrt{\xi + N_g}}(2\pi\sigma^2)^{-\frac{N_g-1}{2}}\exp\left(-\frac{N_g}{2\sigma^2}\left[\frac{\xi}{\xi + N_g}(\mu_g - m_g)^2 + s_g^2\right]\right)\right]
		\\
		\text{where}&& \xi = \frac{\sigma^2}{\sigma_0^2},\qquad m_g = \frac{1}{N_g}\sum_{i=1}^{N_g} x_{g,i}, \qquad s_g^2 = \frac{1}{N_g}\sum_{i=1}^{N_g} x_{g,i}^2 - m_g^2
	\ea

\end{itemize}

\newpage
\subsection{Example: Beta-Binomial}
Life-time performance 10 different boxers are collected. Wins ($k_i$) and losses ($n_i - k_i$) are tallied, and the observed win rate $p_{i,\text{obs}} = k_i / n_i$ is calculated. This is shown below
\begin{figure}[h]
\centering
	\includegraphics[width=0.7\textwidth]{./figs/04-betabinom-data.pdf}
\end{figure}

We would like to determine the win rate of an ``typical boxer'', i.e. the distribution of the win rate $p$. While the observed values $p_\text{obs}$ are good estimates of the individual win rates, 5 boxers played less than 30 matches, while 5 played more than 40, which means the second group provides more information, and their observed win rates need to be taken with more certainty. The Beta-Binomial hierarchical model accounts for this inhomogeneity.
\begin{itemize}
	\item Data: $\{n_i\} = [24, 23, 30, 21, 25, 53, 41, 52, 64, 57]$, $\{k_i\} = [ 10, 13,  9, 10,  9, 51, 28, 37, 59, 45]$, with $N = 10$.
	\item Parameters: $\alpha, \beta > 0$
	\item Model:
	\ba
		&& \log P(D\;|\;\alpha, \beta) = \sum_{i=1}^N \log \Big(\text{Beta-Binom}(k_i\;|\;n_i, \alpha, \beta)\Big)
		\\
		&\text{where}&
		\log\Big(\text{Beta-Binom}(k\;|\;n, \alpha, \beta)\Big) = -f(n, \alpha+\beta) + f(k, \alpha) + f(n-k, \beta),
		\\
		&\text{where}&
		f(k, \alpha) = \log\Gamma(k+\alpha) - \log\Gamma(k+1) - \log\Gamma(\alpha)
	\ea
	which can be implemented as 
\begin{lstlisting}[language=python]
from scipy.special import gammaln

def log_three_gamma_term(k, a):
    return gammaln(k+a) - gammaln(k+1) - gammaln(a)

def log_beta_binom(k, n, a, b):
    target = 0
    target += - log_three_gamma_term(n, a+b)
    target += log_three_gamma_term(k, a)
    target += log_three_gamma_term(n-k, b)
    return target

def log_likelihood(k, n, a, b):
    target = 0
    for ki, ni in zip(k, n):
        target += log_beta_binom(ki, ni, a, b)
    return target
\end{lstlisting}

	\item While assuming flat priors for $\alpha$ and $\beta$, we can numerically calculate their joint posterior and means.
\begin{lstlisting}[language=python]
import numpy as np
import pandas as pd

a_arr, b_arr = np.meshgrid(np.linspace(0.1, 20, 100), 
                           np.linspace(0.1, 20, 100))
a_arr = a_arr.flatten()
b_arr = b_arr.flatten()

loglike = []
for a, b in zip(a_arr, b_arr):
    loglike.append(log_likelihood(k, n, a, b))
    
df = pd.DataFrame({
    'a': a_arr,
    'b': b_arr,
    'loglike': loglike
})

df['pstar'] = np.exp(df['loglike'] - df['loglike'].max())
Z = df['pstar'].sum()
df['posterior'] = df['pstar'] / Z

post_a = df.groupby(by='a')['posterior'].sum().reset_index()
post_b = df.groupby(by='b')['posterior'].sum().reset_index()

a_mean = (post_a['posterior'] * post_a['a']).sum()
b_mean = (post_b['posterior'] * post_b['b']).sum()

\end{lstlisting}
\end{itemize}
giving $\mathbb{E}(\alpha\;|\;D) = 4.142$, $\mathbb{E}(\beta\;|\;D) = 2.289$. 

\no Their (marginal) posteriors and the distribution of the win rate (for the mean $\alpha$ and $\beta$ values) are below.
\begin{figure}[h]
\centering
	\includegraphics[width=0.45\textwidth]{./figs/04-betabinom-ab.pdf}
	\includegraphics[width=0.45\textwidth]{./figs/04-betabinom-fit.pdf}
\end{figure}








