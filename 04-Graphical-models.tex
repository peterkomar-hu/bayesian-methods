\newpage
\section{Graphical models}

As the number of variables we wish to model increases the number of different models increases faster than exponentially. A common technique to get a handle on the dependencies of many real-world variables is to organize them into connected structures the components of which are simple and easily interpretable. This allows us to construct hierarchies, a powerful method for synthesizing knowledge.

Representing variables with the nodes and dependencies with the edges of a graph is a powerful method to gain a bird's eye view of the model and reason about statistical dependencies between the variables under different fixed conditions. This section introduces elements of a graphical representation technique using directed, acyclic graphs. (Other methods are undirected graphs and factor graphs.)

\subsection{Elements}
\begin{itemize}
	\item A {\bf directed connection} from variable $x$, to variable $y$, indicates that we intend to describe their joint distribution using the conditional probability $P(y\;|\;x)$, i.e. $P(x, y) = P(y\;|\;x) P(x)$. The graphical representation of this is
	\begin{figure}[h]
	\centering
		\includegraphics[height=3.4mm]{./figs/04-xy.pdf} 
	\end{figure}

	\item A {\bf chain} of connections, which is formed by two directed connections, one from $x$ to $z$ and another from $z$ to $y$, indicates that we intend to describe the joint as $P(x, y, z) = P(y\;|\;z) P(z\;|\;x) P(x)$. This is represented by a the following graph.
	\begin{figure}[h]
	\centering
		\includegraphics[height=3.4mm]{./figs/04-xzy.pdf} 
	\end{figure}

	Note: This graph represents a restriction to the set of distributions for the joint, because the variable $z$ separates $x$ and $y$, indicating that $x$ and $y$ become independent once we fix the value of $z$ (to any value), i.e. $P(x,y\;|\;z) = P(x\;|\;z) P(y\;|\;z)$. This, we write as $y \independent x \;|\;z$. On the other hand, if $z$ is unknown, $x$ and $y$ may be dependent, which we write as $y \not\independent x\; | \; \emptyset$. The $z$ variable in the middle in such situations is called the ``mediator'' between $x$ and $y$.

	\item A {\bf fork} is formed when two connections originate from one variable $x$ and connect to two different variables $y_1$ and $y_2$, indicating that we intend to describe their joint as $P(x, y_1, y_2) = P(y_1\;|\;x) P(y_2\;|\;x) P(x)$. This is represented by a the following graph.
	\begin{figure}[h]
	\centering
		\includegraphics[height=12mm]{./figs/04-xy1y2.pdf} 
	\end{figure}

	Note: This dependency structure implied by this graph (on its own) is identical to the dependency structure of the chain, i.e. $y_1 \independent y_2 \;|\;x$, and $y_1 \not\independent y_2\; | \; \emptyset$, but the interpretation is different. Variables $x$ is interpreted as a ``common cause'' for $y_1$ and $y_2$.

	\item The {\bf collider}, which is formed by directed connections from two variables $x_1$ and $x_2$ to one variable $y$, is the most interesting graphical element. It indicates that we believe that $x_1$ and $x_2$ are independent variables, i.e. the we expect to write the joint as $P(x_1, x_2, y) = P(y\;|\;x_1, x_2) P(x_1) P(x_2)$. This is represented by a the following graph.
	\begin{figure}[h!]
	\centering
		\includegraphics[height=12mm]{./figs/04-x1x2y.pdf} 
	\end{figure}

	Note: By definition, this dependency structure implies $x_1 \independent x_2\;|\;\emptyset$, however the same does not hold if the value of $y$ is fixed, $x_1 \not\independent x_2\;|\;y$. This effect is called ``explaining away''.
	
\end{itemize}

\subsection{General rules}

\no {\bf Converting between graph and formula}

The general rules for converting between a graphical representation and the formula of the joint distribution are the following
\begin{itemize}
	\item Each variable $x$ (node of the graph) contributes a factor $P(x\;|\ldots)$ to the formula. The only question is what should go in place of $\ldots$ in the condition part.

	\item If the node has no incoming edges, the condition part should be left empty, i.e. the factor is $P(x\;|\;\emptyset) = P(x)$.

	\item If the node has incoming connections, the variables from which the connections originate (``parents'') should be listed in the condition part, i.e. the factor is $P(x\;|\;\text{parents of }x)$.
\end{itemize}


\no {\bf d-separation}

Two variables are said to be ``d-separated'' if their independence (under specific conditions) is implied by the graph structure. For the general case of trying to determine independence between $x$ and $y$ under the condition of keeping variables $z_1, z_2, \ldots z_c$ fixed, the steps are the following.
\begin{enumerate}
	\item List all possible paths between $x$ and $y$ through the graph disregarding the directions of the connections (i.e. moving in the opposite direction of the connections is also allowed). Each node is allowed to be visited at most once in every path.

	\item For each node on each path, mark the way it is traversed with respect to the direction of the connections: mediator ($\leftarrow\leftarrow$ or $\rightarrow\rightarrow$), fork ($\leftarrow\rightarrow$) or collider ($\rightarrow\leftarrow$). (Note: It is possible for a node with three or more connections to be traversed differently by different paths.)

	\item Disregard all paths of which any of the mediator or fork nodes is among the $\{z_1, z_2, \ldots z_c\}$ variables, on which we are conditioning. These paths are ``blocked''.

	\item On the remaining paths, investigate the colliders. Going against our intuition, a collider blocks the path if {\bf neither} itself {\bf nor} any of its descendents is among $\{z_1, z_2, \ldots z_c\}$. (E.g. if we do not condition on any variable, then all paths that traverse at least one node in collider fashion are blocked.) Disregard these paths.

	\item If all paths are blocked, in one way or another, $x$ and $y$ are conditionally independent, conditioned on the variables $z_1, z_2, \ldots z_c$, which we write as $x \independent y\;|\; z_1, z_2, \ldots z_c$.
\end{enumerate}

\no {\bf Examples}
\begin{itemize}
	\item $P(a, b, c, d, e) = P(a)P(d) P(b\;|\;a,d) P(c\;|\;b) P(e\;|\;c) P(f\;|\;c)$ is represented by 

	\begin{figure}[h!]
	\centering
		\includegraphics[height=15mm]{./figs/04-abcdef.pdf} 
	\end{figure}
	Some examples of d-separation are $e \independent f \;|\; c$,\quad $e \independent a \;|\; b$ and $a \independent d \;|\;\emptyset$, (but $a \not\independent d \;|\;b$ and $a \not\independent d \;|\;f$).

	\item $P(a, b, c, d, e, f) = P(a) P(c\;|\;a) P(b\;|\;a) P(f\;|\;c) P(d\;|\;b, c) P(e\;|\;d)$ is represented by

	\begin{figure}[h!]
	\centering
		\includegraphics[height=20mm]{./figs/04-abcdef2.pdf} 
	\end{figure}
	Some examples of d-separation are $b \independent f \;|\; a$ and  $b \independent f \;|\; a,c,e$ (but $b \not\independent f \;|\; a,d$ and $b \not\independent f \;|\; a,e$).
\end{itemize}

\newpage
\subsection{Real-life examples}
\begin{itemize}
	\item Fire causes Smoke, Smoke causes Alarm to set off, but given Smoke, there's no correlation between Fire and Alarm, i.e. $\text{Fire}\;\independent\;\text{Alarm}\;|\;\text{Smoke}$. This is represented by a chain
	\begin{figure}[h!]
	\centering
		\includegraphics[height=2.5mm]{./figs/04-fire-smoke-alarm.pdf} 
	\end{figure}

	\item Both rain and the Sprinkler can cause the formation of a Puddle, they are however independent (until we observe the Puddle), i.e. $\text{Rain}\;\independent\;\text{Sprinkler}\;|\;\emptyset$. This is represented by a collider
	\begin{figure}[h!]
	\centering
		\includegraphics[height=12.5mm]{./figs/04-rain-puddle-sprinkler.pdf} 
	\end{figure}

	\item Heat causes both Ice Cream sales and Crime to increase, but once we know there was a heatwave, they become independent, i.e. $\text{Crime}\;\independent\;\text{Ice Cream}\;|\;\text{Heat}$. This is represented by a fork
	\begin{figure}[h!]
	\centering
		\includegraphics[height=10.5mm]{./figs/04-heat-icecream-crime.pdf} 
	\end{figure}

	\item Education affects Political View, which affects both Party membership and Voting behavior. This can be represented as
	\begin{figure}[h!]
	\centering
		\includegraphics[height=14.4mm]{./figs/04-education-vote.pdf} 
	\end{figure}

	\item Education and Experience both affect Salary, but Education also affects Experience. This can be represented as
	\begin{figure}[h!]
	\centering
		\includegraphics[height=18mm]{./figs/04-education-salary.pdf} 
	\end{figure}
\end{itemize}

\subsection{Plate notation}
When a sequence of variables are connected to the rest of the graph in identical fashion, we simplify our notation by placing a representative of the variables inside a box (``on a plate'') in place of the series, and label the box with the length of the sequence.
\begin{figure}[h!]
\centering
	\includegraphics[height=30mm]{./figs/04-plate-notation.pdf}
\end{figure}

\newpage
\subsection{Hierarchical models}
\label{sec:Hiararchical models}
Using the graphical representation, we can define and understand models of multiple levels of hierarchy with ease. 

The usual difficulty that comes with multi-level models is the rapid increase of the number of parameters. If not kept in check, this leads to overfitting (in MLE context) or inefficient sampling (in MCMC context). Once in a while, variables on intermediate levels of a hierarchical model can be marginalized out, giving rise to a log likelihood function with fewer parameters that enable a more efficient and more robust solutions.
\\

\no {\bf Beta-Binomial model}

Often different experimental runs, each of which contain observations with binary outcomes, have similar but not identical success rates. E.g. the win rate of athletes in a specific league is expected to be similar (since they are in the same league), but we certainly can't rule our individual differences. In such a case, the Beta-Binomial model can robustly estimate the distribution of win rates in the league.
\begin{itemize}
	\item We observe $N$ experimental runs, where in the $i$th experiment we find  $k_i$ successes out of $n_i$ attempts. The data is $D = \{(k_i, n_i)\}_{i=1}^N$, where $0 \leq k_i\leq n_i$, with $k_i, n_i \in \mathds{N}$.

	\item Model: 
		\begin{itemize}
			\item One level 1, we model number of successes with individual success rates, $p = \{p_i\}_{i=1}^N$, where $p_i \in [0,1]$, and assume that measurements are independent (given $p$). This leads to $k_i$ being  distributed as $P(k_i\;|\;n_i, p_i) = \text{Binomial}(k_i\;|\;n_i, p_i)$.
			\item On level 2, we assume that the success rates come from a Beta distribution with parameters, $\alpha, \beta$ (both $>0$), i.e. define $P(p_i\;|\;\alpha, \beta) = \text{Beta}(p_i\;|\;\alpha, \beta)$.
		\end{itemize}
		This hierarchy can be represented as
		\begin{figure}[h!]
		\centering
			\includegraphics[height=26mm]{./figs/04-BetaBinomial.pdf}
		\end{figure}

	\item If we treat the success rates as hidden data, we can write the likelihood as
	\be
		P(D, p\;|\;\alpha,\beta) = \prod_{i=1}^N\Big[\text{Binom}(k_i\;|\;n_i, p_i) \;\text{Beta}(p_i\;|\;\alpha, \beta)\Big]
	\ee

	\item For this model, we can marginalize out $p$ analytically, yielding a more useful form of the likelihood:
	\ba
		P(D\;|\;\alpha, \beta) &=& \prod_{i=1}^N\left[\int\!dp_i\,\text{Binom}(k_i\;|\;n_i, p_i) \;\text{Beta}(p_i\;|\;\alpha, \beta)\right] = \prod_{i=1}^N\Big[\text{Beta-Binom}(k_i\;|\;n_i, \alpha, \beta)\Big]
		\\
		\text{where}&&
		\text{Beta-Binom}(k\;|\;n, \alpha, \beta) = \frac{\Gamma(n+1)\Gamma(\alpha + \beta)}{\Gamma(n + \alpha + \beta)} \frac{\Gamma(k+\alpha)}{\Gamma(k+1) \Gamma(\alpha)} \frac{\Gamma(n - k + \beta)}{\Gamma(n-k + 1) \Gamma(\beta)}
	\ea
	Using the definition of the beta function, $B(x,y) = \Gamma(x)\Gamma(y) / \Gamma(x + y)$, we can write the log likelihood as 
	\be
		L(\alpha, \beta) = \log(n) +  \log B(n, \alpha+\beta) - \Big[\log(k) + \log B(k, \alpha)\Big]_\text{if $k\neq 0$} - \Big[\log(n-k) + \log B(n-k, \beta)\Big]_\text{if $k\neq n$}
	\ee
	which is numerically more robust to evaluate (because $\log B$ is often implemented directly in numerical packages, e.g. by \texttt{scipy.special.betaln} in python), and statistically more robust to optimize.
\end{itemize}

\newpage
\no {\bf Gamma-Poisson model} (aka. Negative Binomial model)

If we observe multiple sequences of events, each of which is generated by a Poisson process, but we think that the Poisson parameters are not identical, then the Gamma-Poisson model can provide robust estimates of the distribution of Poisson parameters. This is the simplest model for an inhomogeneous population of counts.
\begin{itemize}
	\item We observe a sequence of counts (e.g. number of events), one for each experiment. The data is $D = \{k_i\}_{i=1}^N$, where $k_i$ is the count observed in the $i$th experiment.
	\item Model: 
	\begin{itemize}
		\item On level 1, we model the counts from each experiment with a Poisson distribution $P(k_i\;|\;\lambda) = \text{Poisson}(k_i\;|\;\lambda_i)$. The Poisson  parameters $\lambda = \{\lambda_i\}_{i=1}^N$, $\lambda_i$ > 0, represent the typical number of events in each experiment.
		\item On level 2, we assume that the Poisson parameters come from a Gamma distribution with parameter $\alpha, \beta$ (both $>0$), i.e.  $P(\lambda_i\;|\; \alpha, \beta) = \text{Gamma}(\lambda_i\;|\;\alpha, \beta)$.
	\end{itemize}
	This hierarchy can be represented as 
	\begin{figure}[h!]
		\centering
			\includegraphics[height=26mm]{./figs/04-GammaPoisson.pdf}
		\end{figure}
	\item If we treat $\lambda$ parameters as hidden data, we can express the likelihood as
	\be
		P(D, \lambda\;|\;\alpha, \beta) = \prod_{i=1}^N\Big[\text{Poisson}(k_i\;|\;\lambda_i) \;\text{Gamma}(\lambda_i\;|\;\alpha, \beta)\Big]
	\ee

	\item For this model, we can integrate over $\lambda$ analytically, which produces the marginal likelihood
	\ba
		P(D\;|\;\alpha, \beta) &=& \prod_{i=1}^N\left[\int\!d\lambda_i\,\text{Poisson}(k_i\;|\;\lambda_i) \;\text{Gamma}(\lambda_i\;|\;\alpha, \beta)\right] = \prod_{i=1}^N\Big[\text{Gamma-Poisson}(k_i\;|\;\alpha, \beta)\Big]
		\\
		\text{where} && \text{Gamma-Poisson}(k\;|\;\alpha, \beta) = \frac{\Gamma(k + \alpha)}{\Gamma(k+1)\Gamma(\alpha)} \left(\frac{1}{\beta + 1}\right)^k \left(\frac{\beta}{\beta + 1}\right)^\alpha = 
		\\
		&=&
		\text{NegativeBinom}(k\;|\;r, p) = {k + r -1 \choose k} p^k (1-p)^r,\quad \text{with }r = \alpha, \; p = \frac{1}{\beta + 1}
	\ea
	The log likelihood can be written as
	\be
	L(\alpha, \beta) = \sum_{i=1}^N \left(\alpha \log (\beta) - (\alpha + k_i)\log(\beta + 1) - \Big[\log(k_i) + \log B(k_i, \alpha)\Big]_\text{if $k_i\neq 0$}\right)
	\ee
	which is numerically more robust to evaluate (because $\log B$ is often implemented directly in numerical packages, e.g. by \texttt{scipy.special.betaln} in python), and statistically more robust to optimize.
\end{itemize}

\newpage
\no {\bf Dirichlet-Multinomial model}

In a sequence of experimental runs, each of which contains experiments with more than two possible outcomes, we may believe that the chances of the possible outcomes is similar between runs, but not exactly the same. In such a setting, the Dirichlet-Multinomial model can robustly estimate the population of outcome probabilities.
\begin{itemize}
	\item We perform $N$ experimental runs, in each of which the individual experiments have $M$ possible outcomes. (E.g. a classroom of $N$ children visits an ice cream shop that sells $M$ flavors of ice cream every week.) The $i$th run contains $n_{i}$ attempts, out of which the different outcomes are present $(k_{i,1}, k_{i,2}, \ldots k_{i,M})$ number of times. The data is $D = \{k_i\in\mathds{N}^M\}_{i=1}^N = \{(k_{i,1}, k_{i,2}, \ldots k_{i,M})\}_{i=1}^N$, where $k_{i,j}\in \mathds{N}$ is the number of times outcome $j$ was observed in run $i$, $0\leq k_{i,j} \leq n_i$, and $\sum_{j=1}^M k_{i,j} = n_i$.
	\item Model:
	\begin{itemize}
		\item On level 1, we model each run $i$ with a probability vector $p_i = (p_{i,1}, p_{i,2},\ldots p_{i,M})$, which indicates the probability of each outcome in this run. The full set of such vectors is $p = \{p_i \in \mathds{R}^{M}\}_{i=1}^N = \{(p_{i,1}, p_{i,2},\ldots p_{i,M})\}_{i=1}^N$, where $p_{i,j}>0$, and $\sum_{j=1}^M p_{i,j} = 1,\;\forall i$. Assuming that the experiments are independent (given $p$) leads to $P(k_i\;|\;n_i, p_i) = \text{Multinomial}(k_i\;|\;n_{i},p_i)$.
		\item On level 2 we assume that each probability vector $p_i$ comes from a common Dirichlet distribution with parameters $\alpha = (\alpha_1, \alpha_2, \ldots \alpha_M)$, where each $\alpha_j > 0$. This defines $P(p_i\;|\; \alpha)= \text{Dirichlet}(p_i\;|\;\alpha)$.
	\end{itemize}
	This hierarchy can be represented as 
	\begin{figure}[h!]
		\centering
			\includegraphics[height=26mm]{./figs/04-DirichletMultinomial.pdf}
		\end{figure}
	\item Treating the success probability vectors as hidden data, we can write the likelihood as
	\be
		P(D,p\;|\;\alpha) = \prod_{i=1}^N\Big[ \text{Multinomial}(k_i\;|\;n_i, p_i)\;\text{Dirichlet}(p_i\;|\alpha) \Big]
	\ee
	\item For this model, we can marginalize out $p$, and obtain the likelihood for $\alpha$ as
	\ba
		P(D\;|\;\alpha) &=& \prod_{i=1}^N\left[\int\!dp_i\, \text{Multinomial}(k_i\;|\;n_i, p_i)\;\text{Dirichlet}(p_i\;|\alpha) \right] = \prod_{i=1}^N\Big[\text{Dirichlet-Multinomial}(k_i\;|\;n_i, \alpha)\Big]
		\\
		\text{where} && \text{Dirichlet-Multinomial}(k\;|\;n, \{\alpha_j\}_{j=1}^M) = \frac{\Gamma(n+1)\Gamma(\alpha_\text{tot})}{\Gamma(n + \alpha_\text{tot})} \prod_{j=1}^M \frac{\Gamma(k_j + \alpha_j)}{\Gamma(k_j + 1)\Gamma(\alpha_j)},
	\ea
	where $\alpha_\text{tot} = \sum_{j=1}^M \alpha_j$. The log likelihood can be written as
	\be
		L(\alpha) = \log(n) + \log B(n, \alpha_\text{tot}) - \Big[\log(k_j) + \log B(k_j, \alpha_j)\Big]_\text{if $k_j\neq 0$}
	\ee
	which is numerically more robust to evaluate (because $\log B$ is often implemented directly in numerical packages, e.g. by \texttt{scipy.special.betaln} in python), and statistically more robust to optimize.
\end{itemize}

\newpage
\no {\bf Random Effect Model}

We often group observations by a feature that we suspect to be relevant for determining the measured quantity (e.g. grouping height measurements by gender). Random Effect Model can be used to robustly quantify how much of the populations variance is explained by the feature we used for grouping, i.e. how much smaller is the within-group variance compared to the total variance.
\begin{itemize}
	\item We observe data, which we group into $G$ non-overlapping groups: $D = \{x_g\}_{g=1}^G = \{(x_{g,1}, x_{g,2},\ldots x_{g, N_g})\}_{g=1}^G$, where $x_{g,i} \in \mathds{R}$ is measurement $i$ in group $g$, and groups can be of different sizes $N_g$.
	\item We construct the following hierarchical model:
	\begin{itemize}
		\item On level 1, we assume that each observation comes from its group's distribution $P(x_{g,i}\;|\;\mu_g, \sigma) = \text{Normal}(x_{g,i}\;|\;\mu_g, \sigma^2)$, where $\mu = \{\mu_g\}_{g=1}^G \in \mathds{R}^G$ are the centers for each group and $\sigma^2 > 0$ is the {\bf within-group variance}, shared by the groups.
		\item On level 2, we assume that each group center $\mu_g$ comes from a global distribution, $P(\mu_g\;|\;\mu_0,\sigma_0) = \text{Normal}(\mu_g\;|\;\mu_0, \sigma_0^2)$, where $\mu_0 \in \mathds{R}$ is the global center, and $\sigma_0^2 > 0$ is the {\bf between-groups variance}. (The total variance is $\sigma^2 + \sigma_0^2$.)
	\end{itemize}
	This hierarchy can be represented by the following graph
	\begin{figure}[h]
		\centering
			\includegraphics[height=26mm]{./figs/04-anova.pdf}
	\end{figure}
	\item We regard the group centers as hidden data, and write the likelihood (of $\mu_0, \sigma_0, \sigma$) as
	\be
		P(D, \mu\;|\; \mu_0, \sigma_0, \sigma) = \prod_{g=1}^G\left[\text{Normal}(\mu_g\;|\;\mu_0, \sigma_0^2) \times \prod_{i=1}^{N_g}\text{Normal}(x_{g,i}\;|\;\mu_g, \sigma^2)\right]
	\ee
	\item Integrating over the group centers yields the marginal likelihood
	\ba
		P(D\;|\;\mu_0, \sigma_0, \sigma) 
		&=& 
		\prod_{g=1}^G\intop_{-\infty}^{+\infty}\!d\mu_g\,\left[\text{Normal}(\mu_g\;|\;\mu_0, \sigma_0^2) \times \prod_{i=1}^{N_g}\text{Normal}(x_{g,i}\;|\;\mu_g, \sigma^2)\right] 
		\\
		&=&
		\prod_{g=1}^G\left[ (2\pi \sigma^2)^{-\frac{N_g}{2}} \sqrt{\frac{\sigma^2}{\sigma^2 + N_g\sigma_0^2}} 
		\;\exp\left(-\frac{N_g}{2}\frac{(\mu_0 - m_g)^2}{\sigma^2 + N_g\sigma_0^2} - \frac{N_g s_g^2}{2\sigma^2}\right)\right],
	\ea
	where $m_g = \frac{1}{N_g}\sum_{i=1}^{N_g} x_{g,i}$ and $s_g^2 = \frac{1}{N_g}\sum_{i=1}^{N_g} x_{g,i}^2 - m_g^2$ are the observed average and variance of the observations in group $g$.
	The log likelihood is
	\be
		L(\mu_0,\sigma^2_0, \sigma^2) = \sum_{g=1}^G\left[ -\frac{N_g}{2}\log(2\pi \sigma^2) - \frac{1}{2} \log\left(\frac{\sigma^2}{\sigma^2 + N_g\sigma_0^2}\right)\;-\frac{N_g}{2}\frac{(\mu_0 - m_g)^2}{\sigma^2 + N_g\sigma_0^2} - \frac{N_g s_g^2}{2\sigma^2}\right].
	\ee
	Maximizing this function with respect to $(\mu_0, \sigma_0, \sigma)$ is a more efficient than maximizing $P(D,\mu\;|\;\mu_0, \sigma_0, \sigma)$ with respect to $(\mu, \mu_0, \sigma_0, \sigma)$ and yields more robust estimates.
\end{itemize}

\newpage
\subsection{Example: Beta-Binomial}
The life-time performance of 10 different boxers are collected. Wins ($k_i$) and losses ($n_i - k_i$) are tallied, and the observed win rate $p_{i,\text{obs}} = k_i / n_i$ is calculated. This is shown below
\begin{figure}[h]
\centering
	\includegraphics[width=0.7\textwidth]{./figs/04-betabinom-data.pdf}
\end{figure}

We would like to determine the win rate of a ``typical boxer'', i.e. the distribution of the win rate $p$. While the observed values $p_\text{obs}$ are good estimates of the individual win rates, 5 boxers played up to 30 matches, while 5 played more than 40, which means the second group provides more information, and their observed win rates need to be taken with more certainty. The Beta-Binomial model can accounts for this inhomogeneity.
\begin{itemize}
	\item Data: $\{n_i\} = [24, 23, 30, 21, 25, 53, 41, 52, 64, 57]$, $\{k_i\} = [ 10, 13,  9, 10,  9, 51, 28, 37, 59, 45]$, with $N = 10$.
	\item Parameters: $\alpha, \beta > 0$
	\item Model:
	\ba
		&& \log P(D\;|\;\alpha, \beta) = \sum_{i=1}^N \log \Big(\text{Beta-Binom}(k_i\;|\;n_i, \alpha, \beta)\Big)
		\\
		&\text{where}&
		\log\Big(\text{Beta-Binom}(k\;|\;n, \alpha, \beta)\Big) = -f(n, \alpha+\beta) + f(k, \alpha) + f(n-k, \beta),
		\\
		&\text{where}&
		f(k, \alpha) = \log\Gamma(k+\alpha) - \log\Gamma(k+1) - \log\Gamma(\alpha)
	\ea
	which can be implemented as 
\begin{lstlisting}[language=python]
from scipy.special import gammaln

def log_three_gamma_term(k, a):
    return gammaln(k+a) - gammaln(k+1) - gammaln(a)

def log_beta_binom(k, n, a, b):
    target = 0
    target += - log_three_gamma_term(n, a+b)
    target += log_three_gamma_term(k, a)
    target += log_three_gamma_term(n-k, b)
    return target

def log_likelihood(k, n, a, b):
    target = 0
    for ki, ni in zip(k, n):
        target += log_beta_binom(ki, ni, a, b)
    return target
\end{lstlisting}

	\item While assuming flat priors for $\alpha$ and $\beta$, we can numerically calculate their joint posterior and means.
\begin{lstlisting}[language=python]
import numpy as np
import pandas as pd

a_arr, b_arr = np.meshgrid(np.linspace(0.1, 20, 100), 
                           np.linspace(0.1, 20, 100))
a_arr = a_arr.flatten()
b_arr = b_arr.flatten()

loglike = []
for a, b in zip(a_arr, b_arr):
    loglike.append(log_likelihood(k, n, a, b))
    
df = pd.DataFrame({
    'a': a_arr,
    'b': b_arr,
    'loglike': loglike
})

df['pstar'] = np.exp(df['loglike'] - df['loglike'].max())
Z = df['pstar'].sum()
df['posterior'] = df['pstar'] / Z

post_a = df.groupby(by='a')['posterior'].sum().reset_index()
post_b = df.groupby(by='b')['posterior'].sum().reset_index()

a_mean = (post_a['posterior'] * post_a['a']).sum()
b_mean = (post_b['posterior'] * post_b['b']).sum()

\end{lstlisting}
\end{itemize}
giving $\mathbb{E}(\alpha\;|\;D) = 4.142$, $\mathbb{E}(\beta\;|\;D) = 2.289$. 

\no Their (marginal) posteriors and the distribution of the win rate (for the mean $\alpha$ and $\beta$ values) are below.
\begin{figure}[h]
\centering
	\includegraphics[width=0.45\textwidth]{./figs/04-betabinom-ab.pdf}
	\includegraphics[width=0.45\textwidth]{./figs/04-betabinom-fit.pdf}
\end{figure}








