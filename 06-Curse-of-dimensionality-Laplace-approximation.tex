\section{Curse of Dimensionality, Laplace approximation}
The more parameters a model has the longer it takes to find a MLE solution. Iterative methods (such as EM) can lead to sub-optimal local maxima, and complete discovery of the parameter space becomes prohibitively difficult. Computing the normalization constant for the  posterior is equally difficult, and (unless an analytical solution is known) requires us to rely on approximations, the simplest of which is known as Laplace approximation.

\subsection{High-dimensional example}
Let us observe how the parameter space can reach combinatorial sizes even for a conceptually simple model.
\begin{itemize}
	\item Say, we measure the number of influenza cases $k_t$ at small clinic on each day $t$ of the year $\{1,2,\ldots T\}$ ($T = 365$). The recorded data is $D = \{k_t\}_{t=1}^T$, where $k_t\in\mathds{N}$.
	\item Since the clinic is small, the number of cases in each day is small (often 0). Still we would like to infer when the number of influenza cases reach its peak, and to do this we wish to smoothen the $\{k_t\}$ signal. Let's construct the following hierarchical model for this job:
	\begin{itemize}
		\item On level 1, we assume that each count $k_t$ comes from a Poisson distribution with parameter $\lambda_t$, which we can call the global intensity of influenza on day $t$, 
		\be
			P(k_t\;|\;\lambda_t) = \text{Poisson}(k_t\;|\; \lambda_t).
		\ee
		\item On level 2, we assume that consecutive $\lambda_t$ values are close to each other. Let's assume that the logarithm $\theta_t := \log(\lambda_t)$ is normally distributed around $\theta_{t-1}$ with a fixed variance $\sigma^2$: 
		\be
			P(\theta_t\;|\;\theta_{t-1}) = \text{Normal}(\theta_t\;|\;\theta_{t-1}, \,\sigma^2).
		\ee
		\item We assume a flat prior for $\sigma^2$ and $\theta_1$, i.e. $P(\sigma^2) = \text{const.}$, $P(\theta_1) = \text{const.}$
	\end{itemize}
	\item Putting the above formulas together allows us to write the posterior as
	\be
		P(\theta\;|\;D) = \frac{1}{Z} P\s(\theta\;|\;D) = \frac{1}{Z} \prod_{t=1}^T \Big[ \text{Poisson}(k_t\;|\;\exp(\theta_t)) \; \text{Normal}(\theta_{t}\;|\;\theta_{t-1}, \sigma^2) \Big]
	\ee
	with the understanding that $``\text{Normal}(\theta_{1}\;|\;\theta_{0}, \sigma^2)'' = 1$.
	Here $Z$ is the normalization constant that we need to determine.
	\begin{figure}[h]
	\centering
		\includegraphics[width=0.5\textwidth]{./figs/06-influenza.pdf}
	\end{figure}
	\item A direct numerical calculation of $Z$ would require evaluating $P\s$ on a grid of different $\theta$ values. Even, at the very extreme, when we consider only 2 values for each $\theta_t$, the number of evaluations becomes
	\be
		2^{365} \approx 10^{109} > 10^{86} \text{ (the number of protons in the observable part of the universe),}
	\ee
	which makes it impossible to pursue this strategy.
\end{itemize}

\subsection{Laplace approximation}
Laplace approximation is based on the philosophy that knowing an approximation of the variance of the posterior (even if it is often below the true variance) is better than knowing nothing. 

The MLE/MAP solution reports the mode of the posterior. The Laplace appoximation complements this by fitting a Gaussian to the immediate neighborhood of the mode. The (co)variance of the fitted Gaussian indicates the spread of the posterior (at least near the mode). The logical flow of the Laplace approximation is the following:
\begin{itemize}
	\item Goal: We wish to determine the posterior mean and variance of each every parameter $\theta_t$ of a model.
	\item Challenge: The dimension of $\theta = \{\theta_t\}_{t=1}^T$, i.e. $T$, is too high. We cannot hope to ever finish a direct numerical evaluation of the unnormalized posterior $P\s(\theta\;|\;D)$ on a uniform grid.
	\item Method: We approximate $P\s(\theta\;|\;D)$ near its maximum with a multi-variate normal distribution, whose covariance matches with the ``curvature'' of the unnormalized posterior at the mode.
	\ba
		P\s(\theta\;|\;D) 
		&\approx& 
		\text{Normal}(\theta\;|\;\mu, \Sigma) = \frac{1}{\sqrt{\det(2\pi \Sigma)}} \exp\left[-\frac{1}{2}(\theta - \mu)^\top \Sigma^{-1} (\theta - \mu)\right]
		\\
		\\
		\text{where} 
		&&
		\mu = \amax_\theta \big[\log P\s\big] \qquad \in \mathds{R}^T
		\\
		&& \Sigma = \left[-\frac{d}{d\theta}\frac{d}{d\theta} \log P\s\right]^{-1}_{\theta = \mu} \hspace{1.9mm} \in \mathds{R}^{T\times T}
	\ea
	where $\mu$ can be found with direct numerical or analytical minimization or iteratively with an expectation maximization algorithm, and $\Sigma$ can be evaluated analytically or approximated numerically.
	\item Result: Using the results, $\mu$ and $\Sigma$, we say that they approximate the mean and covariance matrix of the posterior of $\theta$, respectively.
\end{itemize}

\subsection{Example: $(x,y)$ linear regression}
Let's observe the Laplace approximation in action for a 2-to-1 linear model and compare its results with the MLE solution and the exact posterior.
\begin{itemize}
	\item For this toy example, we record the following data: $D = \{D_x, D_y\}$, 
	\begin{itemize}
		\item where $D_x = \{x_i\}_{i=1}^N = [21, 24, 17, 39, 23, 45, 33, 26, 13, 35]$, 
		\item and $\quad D_y = \{y_i\}_{i=1}^N = [22, 27, 22, 29, 26, 36, 30, 26, 15, 37]$
	\end{itemize}
	\begin{figure}[h]
	\centering
		\includegraphics[width=0.37\textwidth]{./figs/06-data.pdf}
\end{figure}
	\item We model $y$ with a normal distribution whose mean $\mu(x)$ depends on $x$.
	\begin{itemize}
		\item The mean $\mu(x)$ is linearly dependent of $x$ from plus an offset term: $\mu(x) = a x + b$.
		\item The variance is assumed to be independent of $x$ (this assumption is called homoscedasticity), allowing us to write the 
		\be
			P(y_i\;|\;x_i, a, b, \sigma^2) = \text{Normal}(y_i\;|\;\mu(x_i), \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left[-\frac{\big(y_i - (ax_i + b)\big)^2}{2\sigma^2}\right].
		\ee
		\item For the sake of simplicity, we assume flat priors for all three parameters, i.e. $P(a, b, \sigma^2) = \text{const.}$
	\end{itemize}
	\item The unnormalized log posterior (up to an additive constant) is
		\be
			\log P\s(a, b, \sigma^2\;|\;D) = \sum_{i=1}^N \log P(y_i\;|\;x_i, a, b, \sigma^2)  = -\frac{N}{2}\log(2\pi \sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^N\Big[y_i - (ax_i + b)\Big]^2
		\ee
	\item The model is simple enough that we can derive the MLE estimates analytically. Setting the first derivatives with respect to $a, b, \sigma^2$ to zero, and solving the resulting system of equations yields
		\be
			\left.
				\begin{array}{l}
				0 = \frac{\partial}{\partial a} \log P\s = \frac{1}{\sigma^2}\left[\sum_i y_i x_i - a \sum_i x_i^2 - b \sum_i x_i\right]
				\\
				0 = \frac{\partial}{\partial b} \log P\s = \frac{1}{\sigma^2}\left[\sum_i y_i - a\sum_i x_i - bN\right]
				\\
				0 = \frac{\partial}{\partial (\sigma^2)} \log P\s = -\frac{N}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_i\Big[y_i - (ax_i + b)\Big]^2
				\end{array}
			\right\}
			\Rightarrow
			\left\{
				\begin{array}{l}
				a_\text{MLE} = \big(\overline{yx} - \overline{y}\,\overline{x}\big) / \big(\overline{x^2} - \overline{x}^2\big)
				\\
				b_\text{MLE} = \overline{y} - a_\text{MLE}\,\overline{x}
				\\
				(\sigma^2)_\text{MLE} = \frac{1}{N}\sum_i\big[y_i - (a_\text{MLE}\,x_i + b_\text{MLE})\big]^2
				\end{array}
			\right.
		\ee
		where
		\be
			\overline{x} = \frac{1}{N}\sum_i x_i = 27.6,\qquad 
			\overline{y} = \frac{1}{N}\sum_i y_i = 27.0,\qquad
			\overline{x^2} = \frac{1}{N}\sum_i x_i^2 = 854.0,\qquad
			\overline{yx} = \frac{1}{N}\sum_i y_i x_i = 798.9,\qquad
		\ee
\begin{lstlisting}[language=python]
import numpy as np

def xy_linear_regression_MLE(x, y):
    N = len(x)
    ev_x = np.mean(x)
    ev_y = np.mean(y)
    ev_xx = np.mean(x * x)
    ev_yx = np.mean(y * x)
    ev_yy = np.mean(y * y)

    a_MLE = (ev_yx - ev_y * ev_x) / (ev_xx - ev_x**2)
    b_MLE = ev_y - a_MLE * ev_x
    sigma2_MLE = 1.0/N * np.sum((y - (a_MLE * x + b_MLE))**2)
    
    return a_MLE, b_MLE, sigma2_MLE
\end{lstlisting}
		giving $a_\text{MLE} = 0.5822$, \quad $b_\text{MLE} = 10.93$, \quad  $(\sigma^2)_\text{MLE} = 7.737$.

\newpage
	\item With the MLE result in tow, we can calculate the Laplace approximation:\\
	First, we calculate all second-order derivatives at the MLE point:
	\ba
		\frac{\partial}{\partial a}\frac{\partial}{\partial a} \log P\s &=& -\frac{N}{\sigma^2} \overline{x^2}
		\\
		\frac{\partial}{\partial b}\frac{\partial}{\partial a} \log P\s = \frac{\partial}{\partial a}\frac{\partial}{\partial b} \log P\s &=& -\frac{N}{\sigma^2}\overline{x}
		\\
		\frac{\partial}{\partial b}\frac{\partial}{\partial b} \log P\s &=& -\frac{N}{\sigma^2}
		\\
		\frac{\partial}{\partial (\sigma^2)}\frac{\partial}{\partial a} \log P\s = \frac{\partial}{\partial a}\frac{\partial}{\partial (\sigma^2)} \log P\s &=& 0
		\\
		\frac{\partial}{\partial (\sigma^2)}\frac{\partial}{\partial b} \log P\s = \frac{\partial}{\partial b}\frac{\partial}{\partial (\sigma^2)} \log P\s &=& 0 
		\\
		\frac{\partial}{\partial (\sigma^2)} \frac{\partial}{\partial (\sigma^2)} \log P\s &=& -\frac{N}{2(\sigma^2)^2}
	\ea
	from which we construct the second derivative (Hessian matrix) at the MLE point, and its inverse:
	\be
		\left.-\nabla\nabla \log P\s\right|_\text{MLE} = 
		\frac{N}{(\sigma^2)_\text{MLE}}
		\threebythreematrix
		{\overline{x^2}}{\overline{x}}{0}
		{\overline{x}}{1}{0}
		{0}{0}{\frac{1}{2(\sigma^2)_\text{MLE}}}
	\ee
\begin{lstlisting}[language=python]
minus_dd_logPstar = N / sigma2_MLE * \
np.array([
    [ev_xx, ev_x, 0],
    [ev_x, 1, 0],
    [0, 0, 1/(2*sigma2_MLE)]
])
Sigma = - np.linalg.inv(minus_dd_logPstar)
\end{lstlisting}
	which yields 
	\be
		\Sigma = \Big[-\nabla\nabla \log P\s|_\text{MLE}\Big]^{-1} = \threebythreematrix
		{0.0839}{-0.2315}{0.0}
		{-0.2315}{7.1634}{0.0}
		{0.0}{0.0}{11.197}.
	\ee
	The diagonal elements are approximations of the (marginal) variances of the three parameters $a, b, \sigma^2$:
	\ba
		\text{Var}(a\;|\;D) &\approx& \Sigma_{1,1} = 0.0839,\\
		\text{Var}(b\;|\;D) &\approx& \Sigma_{2,2} = 7.1634,\\
		\text{Var}(\sigma^2\;|\;D) &\approx& \Sigma_{3,3} = 11.197
	\ea
	The off-diagonal elements approximate the covariances:
	\ba
		\text{Cov}(a, b\;|\;D) &\approx & \Sigma_{1,2} = -0.2315, \\ 
		\text{Cov}(a, \sigma^2\;|\;D) &\approx & \Sigma_{1,3} = 0.0, \\
		\text{Cov}(b, \sigma^2\;|\;D) &\approx & \Sigma_{2,3} = 0.0
	\ea

	\item We can plot the MLE, Laplace and exact posteriors of $a, b, \sigma^2$. The MLE result does not hold information about the spread, while the Laplace approximation gets the order of magnitude of the variance right, but still underestimated it. The plot of $\sigma^2$ shows that neither the MLE or the Laplace method captures the skewness of the posterior.
\begin{figure}[h]
	\centering
		\includegraphics[width=0.40\textwidth]{./figs/06-a.pdf}
		\includegraphics[width=0.40\textwidth]{./figs/06-b.pdf}\\
		\includegraphics[width=0.40\textwidth]{./figs/06-sigma2.pdf}
\end{figure}
\newpage
	\item We can also show the predictive distributions on top of the data points. On the left, we plot the MLE result. On the right, we plot predictive distributions for 20 samples from the Gaussian of the Laplace approximation.
\begin{figure}[h]
	\centering
		\includegraphics[width=0.81\textwidth]{./figs/06-linear-regression.pdf}
\end{figure}
\end{itemize}











